{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7736a0a1-ae30-4a26-9c1a-096f14ff37db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "LOADING DATA BY USING SPARK FTROM **CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50d0b135-6c0e-40b5-aba4-d2a8f8b00c01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " #By using Key Vault secrets \n",
    "\n",
    "client_id = dbutils.secrets.get(scope = 'scope-workshop', key = 'databricks-app-client-id')\n",
    "tenant_id = dbutils.secrets.get(scope = 'scope-workshop', key = 'databricks-app-tenant-id')\n",
    "client_secret = dbutils.secrets.get(scope = 'scope-workshop', key = 'databricks-app-client-secret')\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.mjuanworkshopetl.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.mjuanworkshopetl.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.mjuanworkshopetl.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.mjuanworkshopetl.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.mjuanworkshopetl.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99d82ff5-ecb1-4dae-9c16-8de95e64c74f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_df = spark.read.option(\"header\", True).csv(\n",
    "    \"abfss://bronze@mjuanworkshopetl.dfs.core.windows.net/circuits.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "561a0c38-95e4-43d6-a538-e897b80c1a87",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1768194534111}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(circuits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1e41eaf-859b-4bdc-9efc-62d1b418cb73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(circuits_df.printSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8f30e14-ef2e-423c-ab1c-4b95f9e5f8ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_df = spark.read.option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"abfss://bronze@mjuanworkshopetl.dfs.core.windows.net/circuits.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84cf500c-3849-4ce8-acec-08f63fbb26ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(circuits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311ef8ea-b812-42f6-9964-ae192dc31a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "circuits_schema = StructType(fields=[StructField(\"circuitId\", IntegerType(), False),\n",
    "                                   StructField(\"circuitRef\", StringType(), True),\n",
    "                                   StructField(\"name\", StringType(), True),\n",
    "                                   StructField(\"location\", StringType(), True),\n",
    "                                   StructField(\"country\", StringType(), True),\n",
    "                                   StructField(\"lat\", DoubleType(), True),\n",
    "                                   StructField(\"lng\", DoubleType(), True),\n",
    "                                   StructField(\"alt\", IntegerType(), True),\n",
    "                                   StructField(\"url\", StringType(), True)])\n",
    "\n",
    "circuits_df = spark.read.option(\"header\", \"true\")\n",
    ".schema(circuits_schema)\n",
    ".csv(\"abfss://bronze@mjuanworkshopetl.dfs.core.windows.net/circuits.csv\")\n",
    "display(circuits_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c782258e-ab9d-44a4-b986-a64c53a4bfb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5afcb398-3636-4734-a6bf-5df60979dd30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SELECTING COLUMNS USING SPARK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "031db113-8e45-439c-8bc7-91c704d8271d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuit_selected_df = circuits_df\n",
    ".select(\"circuitId\",\"circuitRef\",\"name\",\"location\",\"country\",\"lat\",\"lng\")\n",
    "display(circuit_selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5deb824-0e1d-498e-a762-b5a89a962c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Selecting columns using pyspark sql, this way allow us \n",
    "# to use functions in those columns, like alias\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "circuit_selected_df = circuits_df.select(col(\"circuitId\")\n",
    "                                         ,col(\"circuitRef\"),col(\"name\")\n",
    "                                         ,col(\"location\"),col(\"country\")\n",
    "                                         .alias(\"race_country\"),col(\"lat\")\n",
    "                                         .alias(\"latitude\"),col(\"lng\")\n",
    "                                         .alias(\"longitude\"))\n",
    "\n",
    "circuit_selected_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e74f4b6-fe72-4b8f-bc0f-72f1f2e9154f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_renamed_df = circuit_selected_df.withColumnRenamed(\"circuitId\",\"circuit_id\") \\\n",
    ".withColumnRenamed(\"circuitRef\",\"circuit_ref\")\n",
    ".withColumnRenamed(\"lat\",\"latitude\")\n",
    ".withColumnRenamed(\"lng\",\"longitude\")\n",
    ".withColumnRenamed(\"alt\",\"altitude\")\n",
    "\n",
    "display(circuits_renamed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3ec24a9-c6de-48cd-9dff-fc3d3fa31852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Adding new column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d92960-de2f-4a5f-99bd-9896d5737e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "circuits_final_df = circuits_renamed_df\n",
    ".withColumn(\"ingestion_date\", current_timestamp())\n",
    "\n",
    "display(circuits_final_df)\n",
    "\n",
    "#withColumn and withColumnRenamed are not pyspar.sql.functions imported. They already are Spark methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe20504a-476b-498e-97c1-32c1ab081c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Writting this dataframe as parquet file in our Data Lake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b2e793-c621-49a2-9f57-4b12e2e0d9ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_final_df.write.mode(\"overwrite\")\n",
    ".parquet(\"abfss://processed@mjuanworkshopetl.dfs.core.windows.net/circuits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17802532-e2d5-4831-9474-f7323b2746d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checking that it was saved successfully in Processed container.\n",
    "\n",
    "spark.read.parquet(\"abfss://processed@mjuanworkshopetl.dfs.core.windows.net/circuits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f3e505-41cd-4e19-916e-32a0222dbd18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Saving this dataframes as Managed tables in both Silver Schema in UC and Silver container in the Storage Account.\n",
    "For doing this, first we need to create the Catalog, Schemas and External Locations in UC.\n",
    "Then, we will be able to save the dataframe by using Spark and SQL as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e25e648b-679d-434a-8d89-c002e8246a01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Creating External Locations to the storage account containers. The Storage Account container must be associated with the Access Connector used to create the Storage Credential in this workspace. An Access Connector can have more than one Storage Account linked to it.\n",
    "\n",
    "CREATE EXTERNAL LOCATION IF NOT EXISTS external_location_bronze_container_form\n",
    "URL \"abfss://bronze@mjuanworkshopetl.dfs.core.windows.net/\"\n",
    "WITH (\n",
    "STORAGE CREDENTIAL `databrickcourse-ext-storage-credential`\n",
    ")\n",
    ";\n",
    "\n",
    "\n",
    "CREATE EXTERNAL LOCATION IF NOT EXISTS external_location_silver_container_form\n",
    "URL \"abfss://silver@mjuanworkshopetl.dfs.core.windows.net/\"\n",
    "WITH (\n",
    "STORAGE CREDENTIAL `databrickcourse-ext-storage-credential`\n",
    ")\n",
    ";\n",
    "\n",
    "\n",
    "CREATE EXTERNAL LOCATION IF NOT EXISTS external_location_gold_container_form\n",
    "URL \"abfss://gold@mjuanworkshopetl.dfs.core.windows.net/\"\n",
    "WITH (\n",
    "STORAGE CREDENTIAL `databrickcourse-ext-storage-credential`\n",
    ")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca7bdb1b-b011-4ae0-b118-40f6afd489f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESC EXTERNAL LOCATION external_location_bronze_container_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c91777-72e1-4fde-8db7-66bcd23a645e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE CATALOG IF NOT EXISTS formula1_project;\n",
    "USE CATALOG formula1_project;\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS bronze\n",
    "MANAGED LOCATION \"abfss://bronze@mjuanworkshopetl.dfs.core.windows.net/\" ;\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS silver\n",
    "MANAGED LOCATION \"abfss://silver@mjuanworkshopetl.dfs.core.windows.net/\" ;\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS gold\n",
    "MANAGED LOCATION \"abfss://gold@mjuanworkshopetl.dfs.core.windows.net/\" ;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b44db2a8-f333-4486-9d11-00152f4a62bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Managed table using SQL and source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7533b620-85cf-4273-bb43-9fbcfddcab64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "--Creating a managed table by using SQL and a file.\n",
    "\n",
    "CREATE OR REPLACE TABLE formula1_project.silver.results_sql_file\n",
    "--LOCATION\n",
    "AS SELECT * FROM read_files(\n",
    "  'abfss://bronze@mjuanworkshopetl.dfs.core.windows.net/circuits.csv',\n",
    "  format => 'csv',\n",
    "  header => true\n",
    "  --, schema => 'circuitId INT, circuitRef STRING, name STRING, location STRING, country STRING'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edbe1933-3509-4e7d-8452-654cba349769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Managed table using SQL and a temporary table from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5281137d-a352-4290-ad31-2db61309c9ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Creating a managed table by using SQL and a dataframe.\n",
    "\n",
    "#1. Create a temporary table from the dataframe:\n",
    " \n",
    "circuits_final_df.createOrReplaceTempView(\"temp_circuits_view\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "558947b9-755e-46a9-a703-061069240309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "--2. Create a managed table by using SQL and the temporary view.\n",
    "\n",
    "CREATE OR REPLACE TABLE formula1_project.silver.results_sql_view\n",
    "--LOCATION\n",
    "AS SELECT * FROM temp_circuits_view\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "297e21a5-fd26-4eb5-91ff-3c419453fc77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "External table using SQL and a source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4112915-faad-410f-af19-403c39b4a160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "--Creating an external table by using SQL and a file.\n",
    "\n",
    "CREATE OR REPLACE TABLE formula1_project.silver.results_sql_file_ext\n",
    "LOCATION 'abfss://silver@mjuanworkshopetl.dfs.core.windows.net/circuits.csv'\n",
    "AS SELECT * FROM read_files(\n",
    "  'abfss://bronze@mjuanworkshopetl.dfs.core.windows.net/circuits.csv',\n",
    "  format => 'csv',\n",
    "  header => true\n",
    "  --, schema => 'circuitId INT, circuitRef STRING, name STRING, location STRING, country STRING'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a751790-b2b8-4c92-bf7d-9d7eccd74e6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Managed table using Spark and a source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a1a9b0-af8a-435c-90f2-a231feb98d3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving the transformed dataframe from a file (look at the begining how to read it) as a managed table.\n",
    "\n",
    "circuits_final_df.write.mode(\"overwrite\") \\\n",
    "  .saveAsTable(\"formula1_project.silver.results_spark_file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70a20695-746c-41a7-b785-04f46e89379e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "External table using Spark and a source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "143d5de1-8643-420e-97e1-703e61727296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_final_df.write.mode(\"overwrite\") \\\n",
    "  .option(\"path\", 'abfss://silver@mjuanworkshopetl.dfs.core.windows.net/results') \\\n",
    "  .saveAsTable(\"formula1_project.silver.results_spark_file_ext\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7818630611970720,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Ingestion CSV files",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
